{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a39542",
   "metadata": {},
   "source": [
    "### PHASE 1---------- Load all required libraries ---------- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad084bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import nltk\n",
    "from pymongo import MongoClient\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('words')\n",
    "nltk.download('punkt') \n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a983dc5",
   "metadata": {},
   "source": [
    "### PHASE 1---------- Loading hyperlinks of best buy headphone categories ---------- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "localpath = r\"C:/Users/user/Desktop/BI CASE STUDY 3/Marimo/Headphone On Sale _ Best Buy Canada.html\"\n",
    "file_url = localpath\n",
    "\n",
    "options = Options()\n",
    "#options.add_argument(\"--headless\")\n",
    "service = Service(r\"C:/Users/user/Desktop/BI CASE STUDY 3/Marimo/Chrome Driver/chromedriver-win64/chromedriver.exe\")\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.get(file_url)\n",
    "\n",
    "\n",
    "links = driver.find_elements('xpath', '//a[@class=\"link_3hcyN inline-block h-full w-full\"]')\n",
    "final_Dat = []\n",
    "for link in links:\n",
    "    print(\"---------------------------------------\")\n",
    "    linktxt = link.text\n",
    "    link = link.get_attribute(\"href\")\n",
    "    print(\"Link Text: \", linktxt)\n",
    "    print(\"URL Text: \", link)\n",
    "    linktxt = linktxt.split('\\n')\n",
    "    prdnm = linktxt[0]\n",
    "    prdprc = linktxt[2]\n",
    "    prdlnk = link+\"/review\"\n",
    "    tmplst = [prdnm, prdprc, prdlnk]\n",
    "    final_Dat.append(tmplst)\n",
    "    print(\"\")\n",
    "\n",
    "time.sleep(2) #IN Seconds\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(final_Dat, columns=[\"Product_Name\", \"Price\", \"Review URL\"])\n",
    "df.to_excel(r\"C:/Users/user/Desktop/BI CASE STUDY 3/Marimo/Output/Fetched_Links.xlsx\", index=False)\n",
    "# -------------------------------------------------------------------------------------------\n",
    "time.sleep(5)\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "service = Service(r\"C:/Users/user/Desktop/BI CASE STUDY 3/Marimo/Chrome Driver/chromedriver-win64/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "revlist = []\n",
    "for index, row in df.iterrows():\n",
    "    url = row[\"Review URL\"]\n",
    "    print(\"Current URL: \", url)\n",
    "    driver.get(url)\n",
    "    time.sleep(random.randint(6, 9)) #Ensures that the links are closed in random time\n",
    "    review = driver.find_elements('xpath', '//div[@class=\"reviewContent_XCspv\"]')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if review:\n",
    "        tmplst=[]\n",
    "        for rev in review:\n",
    "            tmplst.append(rev.text)\n",
    "        revlist.append(tmplst)\n",
    "        time.sleep(1)  \n",
    "    else:\n",
    "        revlist.append(\"No reviews found\")\n",
    "    \n",
    "    #reviewnm = driver.find_elements('xpath', '//span[@class=\"author_20vgR\"]')\n",
    "    time.sleep(random.randint(7, 10))\n",
    "\n",
    "    # ---------- Below are the links used from the html file ---------- #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Save to CSV for viewing/reference purpose ---------- #\n",
    "out_dir = r\"C:/Users/user/Desktop/BI CASE STUDY 3/Marimo/Output\"\n",
    "os.makedirs(out_dir, exist_ok=True)               # creates the folder once\n",
    "csv_path = os.path.join(out_dir, \"fetched_reviews.csv\")\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"[✓] {len(df)} rows written ➜ {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa937c30",
   "metadata": {},
   "source": [
    "### PHASE 2---------- Data preprocessing and applying NLP Techniques---------- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df # we can see the data frame is unstructured and not cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Make sure 'Review' column is a list\n",
    "df[\"Review\"] = df[\"Review\"].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "\n",
    "# Remove unwanted promotion text inside each review\n",
    "df[\"Review\"] = df[\"Review\"].apply(lambda reviews: [r.replace(\"[This review was collected as part of a promotion.]\", \"\").strip() for r in reviews])\n",
    "\n",
    "# Explode 'Review' column to create one row per review\n",
    "df_exploded = df.explode(\"Review\").reset_index(drop=True)\n",
    "\n",
    "# Drop rows where 'Review' is missing, empty, or says No reviews found\n",
    "df_exploded = df_exploded[df_exploded[\"Review\"].notnull()]\n",
    "df_exploded = df_exploded[df_exploded[\"Review\"].str.strip() != \"\"]\n",
    "df_exploded = df_exploded[df_exploded[\"Review\"] != \"No reviews found\"]\n",
    "\n",
    "print(f\"[✓] Reshaped DataFrame: {len(df_exploded)} rows\")\n",
    "\n",
    "print(df_exploded.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db603bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded #Transformed data structure to show unique reviews per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d298d5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  Sentence Segmentation\n",
    "df_exploded[\"Sentences\"] = df_exploded[\"Review\"].apply(lambda text: sent_tokenize(text))\n",
    "\n",
    "#  Word Tokenization\n",
    "def tokenize_sentences(sentences):\n",
    "    word_tokens = []\n",
    "    for sent in sentences:\n",
    "        tokens = word_tokenize(sent)\n",
    "        word_tokens.append(tokens)\n",
    "    return word_tokens\n",
    "\n",
    "df_exploded[\"Word_Tokens\"] = df_exploded[\"Sentences\"].apply(tokenize_sentences)\n",
    "\n",
    "df_exploded.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28860e35",
   "metadata": {},
   "source": [
    "### PHASE 3 ---------- After PHASE 2, we will load reviews in MongoDB for Sentiment Classification ---------- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69dfb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"customer_reviews\"]\n",
    "collection = db[\"reviews\"]\n",
    "\n",
    "# Insert each row from df_exploded\n",
    "for idx, row in df_exploded.iterrows():\n",
    "    review_doc = {\n",
    "        \"product_name\": row[\"Product_Name\"],\n",
    "        \"price\": row[\"Price\"],\n",
    "        \"review_url\": row[\"Review URL\"],\n",
    "        \"review\": row[\"Review\"],               # single reviews\n",
    "        \"sentences\": row[\"Sentences\"],          # list of sentences\n",
    "        \"word_tokens\": row[\"Word_Tokens\"]       # list of word tokens\n",
    "    }\n",
    "    collection.insert_one(review_doc)\n",
    "\n",
    "print(f\"[✓] Inserted {len(df_exploded)} documents into MongoDB.\")\n",
    "\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb851347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sentiment Analysis Based on Word Tokens\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"customer_reviews\"]\n",
    "collection = db[\"reviews\"]\n",
    "\n",
    "# Set up Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Process each document\n",
    "updates = []\n",
    "print(\"[...] Starting sentiment analysis based on word_tokens...\")\n",
    "for i, doc in enumerate(collection.find(), start=1):\n",
    "    word_tokens = doc.get(\"word_tokens\", [])\n",
    "    product = doc.get(\"product_name\", \"N/A\")\n",
    "    review_preview = doc.get(\"review\", \"\")[:60]  # show start of the review\n",
    "    \n",
    "    print(f\"\\n Processing {i}: {product}\")\n",
    "    print(f\"    Review: {review_preview}...\")\n",
    "\n",
    "    if word_tokens:\n",
    "        # Flatten the list of lists\n",
    "        flat_tokens = [token for sentence in word_tokens for token in sentence]\n",
    "        text = \" \".join(flat_tokens)  # recreate a string\n",
    "\n",
    "        # VADER sentiment scoring\n",
    "        score = analyzer.polarity_scores(text)\n",
    "        compound = score[\"compound\"]\n",
    "\n",
    "       # Stronger opinion thresholds\n",
    "        if compound >= 0.2:\n",
    "            sentiment = \"Positive\"\n",
    "        elif compound <= -0.2:\n",
    "            sentiment = \"Negative\"\n",
    "        else:\n",
    "            sentiment = \"Neutral\"\n",
    "\n",
    "        print(f\"    Sentiment Score: {score}\")\n",
    "        print(f\"    ➜ Classified as: {sentiment}\")\n",
    "    else:\n",
    "        sentiment = \"Unknown\"\n",
    "        print(\"No tokens found. Marked as 'Unknown'.\")\n",
    "\n",
    "    updates.append(\n",
    "        {\"_id\": doc[\"_id\"], \"sentiment_token_based\": sentiment}\n",
    "    )\n",
    "\n",
    "# Bulk update back into MongoDB\n",
    "print(\"\\n Updating documents in MongoDB...\")\n",
    "for upd in updates:\n",
    "    collection.update_one(\n",
    "        {\"_id\": upd[\"_id\"]},\n",
    "        {\"$set\": {\"sentiment_token_based\": upd[\"sentiment_token_based\"]}}\n",
    "    )\n",
    "\n",
    "print(f\"\\n[✓] Updated {len(updates)} documents with token-based sentiment.\")\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14310b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating visuals to better understand\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"customer_reviews\"]\n",
    "collection = db[\"reviews\"]\n",
    "\n",
    "# Aggregate sentiment counts\n",
    "sentiment_counts = collection.aggregate([\n",
    "    {\"$group\": {\"_id\": \"$sentiment_token_based\", \"count\": {\"$sum\": 1}}}\n",
    "])\n",
    "\n",
    "labels = []\n",
    "counts = []\n",
    "for entry in sentiment_counts:\n",
    "    labels.append(entry[\"_id\"])\n",
    "    counts.append(entry[\"count\"])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Sentiment Distribution Based on Word Tokens')\n",
    "plt.axis('equal')  # Equal aspect ratio makes the pie circular\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, counts)\n",
    "plt.title('Sentiment Distribution Based on Word Tokens')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e8a43",
   "metadata": {},
   "source": [
    "### PHASE 4---------- Visualisations for questions---------- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aaea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"customer_reviews\"]\n",
    "collection = db[\"reviews\"]\n",
    "\n",
    "\n",
    "# Percentage of Positive and Negative Reviews (Pie Chart)\n",
    "\n",
    "sentiment_counts = collection.aggregate([\n",
    "    {\"$match\": {\"sentiment_token_based\": {\"$in\": [\"Positive\", \"Negative\"]}}},\n",
    "    {\"$group\": {\"_id\": \"$sentiment_token_based\", \"count\": {\"$sum\": 1}}}\n",
    "])\n",
    "\n",
    "labels = []\n",
    "counts = []\n",
    "for entry in sentiment_counts:\n",
    "    labels.append(entry[\"_id\"])\n",
    "    counts.append(entry[\"count\"])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Percentage of Positive and Negative Reviews (Token-Based)')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418bdda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"customer_reviews\"]\n",
    "collection = db[\"reviews\"]\n",
    "\n",
    "\n",
    "# Prepare Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Add custom extra stopwords if you want\n",
    "extra_stopwords = {\"product\", \"one\", \"also\", \"get\", \"would\", \"use\", \"buy\"}\n",
    "stop_words.update(extra_stopwords)\n",
    "\n",
    "\n",
    "# Collect Positive and Negative Tokens\n",
    "positive_tokens = []\n",
    "negative_tokens = []\n",
    "\n",
    "for doc in collection.find({\"sentiment_token_based\": {\"$in\": [\"Positive\", \"Negative\"]}}):\n",
    "    word_tokens = doc.get(\"word_tokens\", [])\n",
    "    if word_tokens:\n",
    "        # Flatten the list of lists and lower-case\n",
    "        flat_tokens = [token.lower() for sentence in word_tokens for token in sentence]\n",
    "        if doc[\"sentiment_token_based\"] == \"Positive\":\n",
    "            positive_tokens.extend(flat_tokens)\n",
    "        else:\n",
    "            negative_tokens.extend(flat_tokens)\n",
    "\n",
    "\n",
    "# Remove Stopwords first\n",
    "positive_tokens_clean = [token for token in positive_tokens if token.isalpha() and token not in stop_words]\n",
    "negative_tokens_clean = [token for token in negative_tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "\n",
    "#  Find actual common words and remove\n",
    "# Turn tokens into Counters\n",
    "positive_counter = Counter(positive_tokens_clean)\n",
    "negative_counter = Counter(negative_tokens_clean)\n",
    "\n",
    "# Find common words\n",
    "positive_set = set(positive_tokens_clean)\n",
    "negative_set = set(negative_tokens_clean)\n",
    "common_words = positive_set.intersection(negative_set)\n",
    "\n",
    "# Calculate total frequency of each common word\n",
    "common_word_frequencies = {}\n",
    "for word in common_words:\n",
    "    total_count = positive_counter.get(word, 0) + negative_counter.get(word, 0)\n",
    "    common_word_frequencies[word] = total_count\n",
    "\n",
    "# Sort common words by frequency (descending)\n",
    "common_words_sorted = sorted(common_word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Remove only top 50% most frequent common words\n",
    "half_common = len(common_words_sorted) // 2\n",
    "words_to_remove = set([word for word, count in common_words_sorted[:half_common]])\n",
    "\n",
    "print(f\"[INFO] Total common words found: {len(common_words)}\")\n",
    "print(f\"[INFO] Removing top 50% ({len(words_to_remove)}) common words based on frequency.\")\n",
    "\n",
    "# Now create unique lists by removing only those\n",
    "positive_unique = [token for token in positive_tokens_clean if token not in words_to_remove]\n",
    "negative_unique = [token for token in negative_tokens_clean if token not in words_to_remove]\n",
    "\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Load set of all English words\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Now filter positive and negative unique tokens\n",
    "positive_unique = [token for token in positive_unique if token.lower() in english_words]\n",
    "negative_unique = [token for token in negative_unique if token.lower() in english_words]\n",
    "\n",
    "print(f\"[INFO] After English filtering: {len(positive_unique)} positive tokens, {len(negative_unique)} negative tokens.\")\n",
    "\n",
    "print(f\"[INFO] {len(positive_unique)} unique positive tokens left after partial removal.\")\n",
    "print(f\"[INFO] {len(negative_unique)} unique negative tokens left after partial removal.\")\n",
    "\n",
    "#Create WordClouds\n",
    "\n",
    "# Positive Word Cloud\n",
    "positive_text = \" \".join(positive_unique)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Unique Positive Words (after stopwords and common words removal)')\n",
    "plt.show()\n",
    "\n",
    "# Negative Word Cloud\n",
    "negative_text = \" \".join(negative_unique)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(negative_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Unique Negative Words (after stopwords and common words removal)')\n",
    "plt.show()\n",
    "\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"customer_reviews\"]\n",
    "collection = db[\"reviews\"]\n",
    "\n",
    "# Key Products with the Most Positive or Negative Feedback (Bar Chart)\n",
    "\n",
    "# Top Products with Positive Feedback\n",
    "top_positive_products = collection.aggregate([\n",
    "    {\"$match\": {\"sentiment_token_based\": \"Positive\"}},\n",
    "    {\"$group\": {\"_id\": \"$product_name\", \"count\": {\"$sum\": 1}}},\n",
    "    {\"$sort\": {\"count\": -1}},\n",
    "    {\"$limit\": 5}\n",
    "])\n",
    "\n",
    "# Top Products with Negative Feedback\n",
    "top_negative_products = collection.aggregate([\n",
    "    {\"$match\": {\"sentiment_token_based\": \"Negative\"}},\n",
    "    {\"$group\": {\"_id\": \"$product_name\", \"count\": {\"$sum\": 1}}},\n",
    "    {\"$sort\": {\"count\": -1}},\n",
    "    {\"$limit\": 5}\n",
    "])\n",
    "\n",
    "# Plot Positive Products\n",
    "products_pos = []\n",
    "counts_pos = []\n",
    "for doc in top_positive_products:\n",
    "    products_pos.append(doc[\"_id\"])\n",
    "    counts_pos.append(doc[\"count\"])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(products_pos, counts_pos, color=\"green\")\n",
    "plt.xlabel(\"Number of Positive Reviews\")\n",
    "plt.title(\"Top 5 Products with Most Positive Feedback\")\n",
    "plt.gca().invert_yaxis()  # Highest at top\n",
    "plt.grid(axis='x')\n",
    "plt.show()\n",
    "\n",
    "# Plot Negative Products\n",
    "products_neg = []\n",
    "counts_neg = []\n",
    "for doc in top_negative_products:\n",
    "    products_neg.append(doc[\"_id\"])\n",
    "    counts_neg.append(doc[\"count\"])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(products_neg, counts_neg, color=\"red\")\n",
    "plt.xlabel(\"Number of Negative Reviews\")\n",
    "plt.title(\"Top 5 Products with Most Negative Feedback\")\n",
    "plt.gca().invert_yaxis()  # Highest at top\n",
    "plt.grid(axis='x')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
